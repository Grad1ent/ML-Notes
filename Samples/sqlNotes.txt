---- DDL vs DML:
DDL: Data Definition Language 		> Define data structure		[Create, Alter, Drop, Truncate, Comment, Rename]
DML: Data Manipulation Language		> 							[Select, Insert, Update, Delete, Merge, Call, Explain Plan, Lock Table]
DQL: Data Query Language			> Subset of DML				[Select, Show, Explain, Help]
DCL: Data Control Language			> Rights and permissions	[Grant, Removke]
TCL: Transaction Control Language	> 							[Commit, Rollback, SavePoint, Set Transaction]

---- SQL examples:
https://docs.oracle.com/cd/E10405_01/appdev.120/e10379/ss_oracle_compared.htm

-- T-SQL (Transact SQL) vs PL/SQL (Procedural Langiage SQL):
SELECT getdate() [FROM]			SELECT SYSDATE FROM DUAL
USE database_name				CONNECT user_name/password; SET role;
SELECT ... INTO ...				INSERT INTO ... SELECT ...
INSERT [INTO] ...				INSERT INTO ...
WHERE col = NULL				WHERE col IS NULL
WHERE NULL = NULL > true		WHERE NULL = NULL > false

DISTINCT 	eliminates the duplicate rows
UNION (distinct row from either query) vs UNION ALL (all rows from both queries)
EXISTS (stop scanning after 1st row found) vs. IN (scans all rows):
	SELECT ... FROM ... WHERE EXISTS (SELECT ... FROM ... WHERE ...)
	SELECT ... FROM ... WHERE ... IN (SELECT .... FROM .... WHERE ...)

Aggregate functions: MIN, MAX, SUM, AVG, COUNT, VARIANCE, STDDEV:
	SELECT col, Count(*) FROM tab GROUP BY col HAVING COUNT(*) > 5 ORDER BY 1 DESC

Error handling	> https://docs.oracle.com/cd/B13789_01/appdev.101/b10807/07_errs.htm :
system defined exceptions:	NO_DATA_FOUND, NOT_LOGGED_ON, VALUE_ERROR, ZERO_DIVIDE
user defined exceptions:	
DECLARE 
   my_exception EXCEPTION; 
BEGIN 
   If (...) Then RAISE my_exception
EXCEPTION 
   WHEN ZERO_DIVIDE THEN  
      exception1-handling-statements  
   WHEN my_exception THEN  
      exception3-handling-statements 
END;

---- Transactions:
ACID: Set of properties of database transactions intended to guarantee validity even in the event of errors, power failures, etc.
- atomic: entire transaction always succeeds or fails as one unit of work and never left in a half-completed state
- consistent: always leave the data in a valid state
- isolated: concurrent execution of transactions leaves the database in the same state that would have been obtained if the transactions were executed sequentially + incomplete transaction might not even be visible to other transactions
- duralibility: once a transaction has been committed, it will remain committed even in the case of a system failure




---- Data normalization: Process of structuring a relational database in accordance with a series of normal forms in order to reduce data redundancy and improve data integrity
1NF: atomic columns = single values > no collectons or coma-separated values, etc.
2NF: no partial key dependencies
3NF: no transitive dependencies

"normalized" = 3NF



---- Data warehounse: A data warehouse is a centralized repository of integrated data from one or more disparate sources. Data warehouses store current and historical data and are used for reporting and analysis of the data.
Def: Turn massive amounts of data from operational systems into a format that is easy to understand
[Azure Data Warehouse, Apach Hive on HDInsight, Interactive Query (Hive LLAP) on HDInsight]

Fact table: agregated data (measurements (quantity, amount), metrics or facts of a business process)	[additive, semi additive, none-additive]
Dimension table: description of the data

Star schema:			fact table >> dimension tables (not normalized)
Snowflake schema:		fact table >> dimension tables (normalized) >> dimentions tables (normalized)			(normalization = snowflaking)
Galaxy schema (or fact constellaation schema): fact table >> dimensions tables (shared) << fact table)
Star cluster schema: 	transitive table > [customer] - [< order >] - [product]

Slowly changing dimension (SCD): specify the data warehouseâ€™s response to operational attribute value changes:
Type 0: Retain Original		(attributes never change: date of birth)								[Fixed Dimension]
Type 1: overwrite			(not track historical data)												[No History]
Type 2: add new row			(tracks historical data by creating multiple records)					[Row Versioning]
Type 3: add new attribute	(tracks changes using separate columns and preserves limited history)	[Previous Value column]
Type 4: add history table	(sing "history tables", where one table keeps the current data, and an additional table is used to keep a record of some or all changes)	[History Table]

Type 6: Combined Approach [Hybrid SCD: 1, 2, 3]

---- OLTP vs OALP:
-- OLTP: Online transaction processing > Management system of transactional data, efficiently process, store and query transactional data (efficiency due to normalization = avoids extra processing redundant data)
[Azure SQL Database, SQL Server in Azure VM, Azure Database for MySQL, Azure Database for PostgreSQL]

-:
handling aggregates over large amounts of data
nalytics and reporting on data that is highly normalized, the queries tend to be complex
Storing the history of transactions = slow query performance

Ref: https://docs.microsoft.com/en-us/azure/architecture/data-guide/relational-data/online-transaction-processing

-- OLAP: Online analytical processing > System designed to help extract BI information from the data in a highly performant way. Optimized for heavy read, low write workloads.
[SQL Server Analysis Services (SSAS), Azure Analysis Services, SQL Server with Columnstore indexes, Azure SQL Database with Columnstore indexes]

Semantic modeling: columns are renamed to more user-friendly names + query data without performing aggregates and joins over the underlying schema + removes defs. of underlying data structures
[1] Tabular:			reliational modeling constucts > model, tables, columns
[2] Multidimensional:	traditional modeling constucts > cubes, dimensions, measures
[Data source] > [Data warehounse] > [Semantic layer] > [Reporting and analysis]
									[    OLAP      ]
Ref: https://docs.microsoft.com/en-us/azure/architecture/data-guide/relational-data/online-analytical-processing



---- ETL vs ELT:
-- ETL: Extract, transform, and load > Data pipeline used to collect data from various sources, transform the data according to business rules, and load it into a destination data store.
[Extract: Data source] > [Transform: Transformation engine] > [Load: Target store]

Tools: SQL Server Integration Services (SSIS)

-- ELT: Extract, load and transform > Transformation occurs in the target data store, instead of using a separate transformation engine
[Extract: Data source] > [Load + Transform: Target store]

---- RDBMS workloads: Relational data
[OLTP] > [Data warehouse] > [OLAP] > [Reporting]
[              ETL               ]

---- Data Management patterns:
Cache-Aside											> Load data on demand into a cache from a data store
CQRS: Command and Query Responsibility Segregation	> Segregate operations that read data from operations that update data by using separate interfaces.
Event Sourcing										> Use an append-only store to record the full series of events that describe actions taken on data in a domain.
Index Table											> Create indexes over the fields in data stores that are frequently referenced by queries.
Materialized View									> Generate prepopulated views over the data in one or more data stores when the data isn't ideally formatted for required query operations.
Sharding											> Divide a data store into a set of horizontal partitions or shards.
Static Content Hosting								> Deploy static content to a cloud-based storage service that can deliver them directly to the client.
Valet Key											> Use a token or key that provides clients with restricted direct access to a specific resource or service.

Ref.: https://docs.microsoft.com/en-us/azure/architecture/patterns/category/data-management

Types of partitioning: range (key values like data), hash (fixed number of partitions), list (group unordered rows), composit (mix of range, hash or list)

Local partitioned index:	all keys in a particular index partition refer only to rows stored in a single underlying table partition
Nonpartitioned Indexes:		used on nonpartitioned tables in data warehouse environments and in general to enforce uniqueness if the status of a unique constraint is required to be enforced in a data warehousing environment
Global partitioned index:	keys in a particular index partition may refer to rows stored in multiple underlying table partitions or subpartitions

Primary key: column or combination of columns that contain values that uniquely identify each row in the table
Foreign key: column or columns that hold the primary key value for one table are referenced by the column or columns in another table

Clustered index: defines the order in which data is physically stored in a table
Nonclustered index: key value entry has a pointer to the data row that contains the key value
Columnstore index: technology for storing, retrieving, and managing data by using a columnar data format, called a columnstore (data logically organized as a table with rows and columns, and physically stored in a column-wise data format)

---- None-relational data (NoSQL): None tabular schema of rows and columns
-- Document data store: set of named string fields and object data values (JSON form):
[Azure Cosmos DB]
Key:	Document (JSON form):
- uid		- scalar item (number) or list of partent-child collecion
			- data: xml, yaml, json, bson, text

-- Column-family data store: data organized into columns and rows:
[Cosmos DB Cassandra API, HBase in HDInsight]
Customer Id:		Column-family: Identity
- uid				First name: ...
					Last name: ...

-- Key/Value data store: large hash table
Key:	Value:
- uid	- 00101110...
[Azure Cosmos DB Table API, Azure Cache for Redis, Azure Table Storege]

-- Graph data stores: nodes = entities, edges = directed relationships between entities.


Ref.: https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/non-relational-data



---- Time series data: set of values organized by time (ensor data, stock prices, stream data, application telemetry) > Can be analyzed for historical trends, real-time alerts, or predictive modeling
[Data sources] -- real-time message ingestion --> [Azure Time Series Insight]:
												  [Stream processing] + [Analytical data store] + [Analysis and reporting]

[Data sources] > [Steam buffer: IoT Hub, Event Hub, Kafka on HDInsight] > [Store: HBase, Azure Cosoms DB, Data Lake, Blob storage] > [Present: Power BI, Azure Data Exploer, OpenTSDB in HBase]
																																   > [Predict: ML]
Ref: https://docs.microsoft.com/en-us/azure/architecture/data-guide/scenarios/time-series



---- Big data worloads: Complex and large amount of non-relational data > key-value, JSON, time series (batch or real time) (Query: Spark, Hive, or PolyBase)
[Ingestion] > [Processing] > [Storage] > [ML] > [BI]
[              ELT					 ]

-- Batch processing: processing of big data sources at rest
[Data source] > [Store: Blob, DL, SQL, Cosmos] -- iterative --> [Process: U-SQL, Hive, Pig, Spark] -- iterative --> [Analytic store: Warehouse, Spark, HBase, Hive] > [Analyse: Azure Analysis Services, BI]
Ref: https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/batch-processing

Apache Spark (real-time data processing) (RDD: Resilient Distributed Datasets):
distributed data processing framework that enables large-scale data analytics by coordinating work across multiple processing nodes in a cluster

Hadoop MapReduce (batch processing) (HDFS: Hadoop Distributed File System):
framework for processing and generating big data sets with a parallel, distributed algorithm on a cluster (native, close nodes) or a grid (geo-distributed heterogenous hw)
map:		apply function to the local data and store result
shuffle:	distribute result with key to parallel processing
reduce:		process result per key in parallel

-- Real time processing: processing of big data in motion
[Data source] > [Ingest: IoT\Event Hub, Kafka] > [Process: Stream Analytics, Storm, Spark Streaming] > [Analytic store: Warehouse, Spark, HBase, Hive] > [Analyse: Azure Analysis Services, BI]
Ref: https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/real-time-processing

-- Lambda architecture:
[unified log] > [hot-path: speed layer (real-time view)] > [Analitycs client]
			  > [cold-path: batch layer (master data) -> serving layer (batch view)] > [Analytics client]

[1] cold-path > batch layer: batch processing of raw data and store in batch-view
[2] hot-path > speed layer: low-latency real-time view
[3] serving layer > indexing batch-view

Ref: https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/

-- Kappa architecture: data flows through a single path, using a stream processing system
[unified log] > [speed layer (real-time views)] > [Analitycs client]
			  >	[long term store (master data)] > [speed layer to re-compute]

Ref: https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/

-- IoT: data is sent from low-latency environments by thousands or millions of devices, requiring the ability to rapidly ingest the data and process accordingly
flow: see Real time processing

---- Data analytics and reporting technologies:
-- Power BI: Suite of business analytics tools (collection of software services, apps, and connectors) to produce reports and publish them to the organization > create personalized dashboards, with governance and security built in.
Ref: https://docs.microsoft.com/en-us/power-bi/fundamentals/power-bi-overview

Components:
[1] Power BI Desktop: Windows desktop application
[2] Power BI service: Online SaaS service
[3] Power BI Report Server: Publish Power BI reports to an on-premises report server, after creating them in Power BI Desktop
[4] Power BI mobile apps for Windows, iOS, and Android devices

Data sources (> .PBIDS file):
- All				[..]
- File				[Excel, Text/CSV, XML, JSON, Folder, PDF, SharePoint folder]
- Database			[SQL Server database, Access database, SQL Server Analysis Services database, Oracle database, IBM DB2 database, IBM Informix database (Beta), IBM Netezza, MySQL database, PostgreSQL database, Sybase database, Teradata database, SAP HANA database, SAP Business Warehouse Application Server, SAP Business Warehouse Message Server, Amazon Redshift, Impala, Google BigQuery, Vertica, Snowflake, Essbase, AtScale cubes, BI Connector, Data Virtuality LDW (Beta), Denodo, Dremio., Exasol, Indexima (Beta), InterSystems IRIS (Beta), Jethro (Beta), Kyligence, MarkLogic]
- Power Platform	[Power BI datasets, Power BI dataflows, Common Data Service, Power Platform dataflows]
- Azure				[Azure SQL Database, Azure SQL Data Warehouse, Azure Analysis Services database, Azure Database for PostgreSQL, Azure Blob Storage, Azure Table Storage, Azure Cosmos DB, Azure Data Lake Storage Gen2, Azure Data Lake Storage Gen1, Azure HDInsight (HDFS), Azure HDInsight Spark, HDInsight Interactive Query, Azure Data Explorer (Kusto), Azure Cost Management]
- Online Services	[SharePoint Online List, Microsoft Exchange Online, Dynamics 365 (online), Dynamics NAV, Dynamics 365 Business Central, Dynamics 365 Business Central (on-premises), Microsoft Azure Consumption Insights (Beta), Azure DevOps (Boards only), Azure DevOps Server (Boards only), Salesforce Objects, Salesforce Reports, Google Analytics, Adobe Analytics, appFigures (Beta), Data.World - Get Dataset (Beta), GitHub (Beta), LinkedIn Sales Navigator (Beta), Marketo (Beta), Mixpanel (Beta), Planview Enterprise One - PRM (Beta), Planview Projectplace (Beta), QuickBooks Online (Beta), Smartsheet, SparkPost (Beta), SweetIQ (Beta), Planview Enterprise One - CTM (Beta), Twilio (Beta), tyGraph (Beta), Webtrends (Beta), Zendesk (Beta), Asana (Beta), Dynamics 365 Customer Insights (Beta), Emigo Data Source, Entersoft Business Suite (Beta), FactSet Analytics (Beta), Industrial App Store, Intune Data Warehouse (Beta), Microsoft Graph Security (Beta), Product Insights (Beta), Quick Base, TeamDesk (Beta), Workplace Analytics (Beta), Projectplace for Power BI (Beta), Webtrends Analytics (Beta), Zoho Creator (Beta)]
- Other				[Web, SharePoint list, OData Feed, Active Directory, Microsoft Exchange, Hadoop File (HDFS), Spark, Hive LLAP (Beta), R script, Python script, ODBC, OLE DB, BI360 - Budgeting & Financial Reporting (Beta), Cognite Data Fusion (Beta), FHIR, Information Grid (Beta), Jamf Pro (Beta), MicroStrategy for Power BI, Paxata, QubolePresto (Beta), Roamler (Beta), Siteimprove, SurveyMonkey (Beta), TIBCO(R) Data Virtualization (Beta), Vena (Beta), Workforce Dimensions (Beta), Zucchetti HR Infinity (Beta), Tenforce (Smart)List, Shortcuts Business Insights (Beta), Vessel Insight (Beta), Blank Query]
Ref: https://docs.microsoft.com/en-us/power-bi/desktop-data-sources
Ref: https://docs.microsoft.com/en-us/power-bi/power-bi-data-sources

-- Jupiter Notebooks:  Provide a browser-based shell that lets data scientists create notebook files that contain Python, Scala, or R code and markdown text, making it an effective way to collaborate by sharing and documenting code and results in a single document.
Usually perconfigured in HDInsight clusters, such as Spark or Hadoop
Ref: https://notebooks.azure.com/

-- Azure Notebooks: Online Jupyter Notebooks-based service that enables data scientists to create, run, and share Jupyter Notebooks in cloud-based libraries

-- Tableau: Interactive data visualization software
Ref: https://www.tableau.com/learn/get-started

Components:
[0] Tableau Reader:		Free app to read visualization from [1]
[1] Tableau Desktop:	Building visualization and dashboards
[2] Tableau Public:		Free sharing platform						https://public.tableau.com/en-us/gallery/?tab=viz-of-the-day&type=viz-of-the-day
[3] Tableau Online:		Online SaaS reporting service
[4] Tableau Server:		On-prem report server
[5] Tableau Prep:		Visual form of praparation of the data into analytics
[5.1] Tableau Prep Builder: data flow designer
[5.2] Tableau Prep Connector: share and manage
[6] Tableau Mobile



---- Dictionary:
Data integration patterns: Migration, Broadcast, Bi-directional sync (union), Correlation, Aggregation
Apache Parquet:				Open source, column-oriented data file format designed for efficient data storage and retrieval
Data warehouse:				Centralized repository of integrated data from one or more disparate sources (current and historical data in relational tables that are organized into a schema that optimizes performance for analytical queries).
Data lake:					Storage repository that holds large amounts of data in native, raw formats, as blobs or files.
Delta lake: 				Open-source storage layer that adds relational database semantics to Spark-based data lake processing
Lake database:				Provides a relational metadata layer over one or more files in a data lake.
Data lakehouse:				Open data management architecture that combines the flexibility, cost-efficiency, and scale of data lakes with the data management and ACID transactions of data warehouses, enabling business intelligence (BI) and machine learning (ML) on all data.
	Medalion arch:			Describes a series of data layers that denote the quality of data stored in the lakehouse
		raw (bronze):		unvalidated data
		enriched (silver):	validated, enriched version of our data that can be trusted for downstream analytics
		curated (gold):		highly refined and aggregated, containing data that powers analytics, machine learning, and production applications (knowledge rather than information)

Azure Synapse Analytics:	Integrated analytics service that brings together a wide range of commonly used technologies for processing and analyzing data at scale. 
	Serverless SQL pool: on-demand SQL query processing, primarily used to work with data in a data lake
	Dedicated SQL pool:  enterprise-scale relational database instances used to host data warehouses in which data is stored in relational tables
	Spark pool:			 runtime for Spark operations
Apache Spark:				Distributed, parallel data processing framework that takes advantage of in-memory processing and a distributed file storage, coordinating work across multiple processing nodes in a cluster
Azure Databricks:			Apache Spark-based analytics platform optimised for Azure
	Unity catalog:		Centralized access control, auditing, lineage, and data discovery capabilities across Azure Databricks workspaces
	(user management + metastore)
Apache Hive: 	(Facebook, declarative, HiveQL) bilt on the top of Hadoop and is used to process structured data in Hadoop
Apache Impala: 	Open-source massively parallel processing SQL query engine for data stored in a computer cluster running Apache Hadoop
Apache Pig:		(Yahoo, procedural) Abstract over MapReduce. Pig is used to perform all kinds of data manipulation operations in Hadoop.